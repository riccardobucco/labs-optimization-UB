\section{Gradient descent methods}
    \subsection{A simple quadratic function}
        Let's \(\vec{x} \in \R^2\), \(\vec{x} = (x_1, x_2)^T\). I start focusing on a two dimensional function:
        \[f(\vec{x}) = x_1^2 + x_2^2\]
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-simple-function-contours.png}
            \caption{Contour plot of the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)}
            \label{gradient-descent-simple-function-contours}
        \end{figure}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-simple-function-surface.png}
            \caption{Graph of the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)}
            \label{gradient-descent-simple-function-surface}
        \end{figure}
        I can plot the function to have an idea of its main features (see figures \ref{gradient-descent-simple-function-contours} and \ref{gradient-descent-simple-function-surface}). You can easily see that there is a local minimum: how can I find it? The easiest way is by performing a gradient descent. How does this method work? You just have to start with a guess \(\vec{x}^0\) for a local minimum of \(f(\vec{x})\) and than you have to take steps proportional to the negative of the gradient of the function at the current point. In other words, you have to consider the sequence \(\vec{x}^0, \vec{x}^1, ..., \vec{x}^k, \vec{x}^{k+1}, ...\) such that
        \[\vec{x}^{k+1} = \vec{x}^k - \alpha^k \nabla f(\vec{x}^k)\]
        \(\alpha^k\) plays an important role since I would like to find the minimum as fast as I can. At the moment, let's keep it constant.\par
        Let's perform now some experiments.
        \begin{enumerate}
            \item In the first experiment I'm going to find a local minimum of the function \(f(\vec{x})\) using the gradient descent method with a constant value \(\alpha^k = 0.1\). I'm also going to try with different starting points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-1st-experiment.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha^k = 0.1\) and performing \(100\) iterations}
                \label{gradient-descent-1st-experiment}
            \end{figure}
            As you can see in figure \ref{gradient-descent-1st-experiment}, the gradient descent algorithm converges to the minimum, no matter what the starting point is. It's also possible to see that we don't need \(100\) iterations to get really close to the minimum: in the next experiments we could try to find a way to stop the algorithm when it's close to the real minimum.
            \item In the second experiment I'm going to try different values of \(\alpha^k\) in order to minimize the function \(f(\vec{x})\) using the gradient descent method. Let's start using \(\alpha^k = 1\). The starting points are the same that I used in the first experiment: \(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-2nd-experiment-alpha-1.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha^k = 1\) and performing \(100\) iterations}
                \label{gradient-descent-2nd-experiment-alpha-1}
            \end{figure}
            As you can see in figure \ref{gradient-descent-2nd-experiment-alpha-1}, the method doesn't converge, no matter what is the starting point. Indeed, if \(\vec{x}^0\) is the starting point, the sequence that you get applying the gradient descent method (with \(\alpha^k = 1\)) is \(\vec{x}^0, -\vec{x}^0, \vec{x}^0, -\vec{x}^0, ...\). Moreover, if we use \(\alpha^k = 1.1\) to minimize the function \(f(\vec{x})\), the gradient descent method diverges, moving away from the minimum point.
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-2nd-experiment-alpha-1-1.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from the point \(\vec{x}^0 = (4,4)^T\), using \(\alpha^k = 1.1\) and performing \(12\) iterations}
                \label{gradient-descent-2nd-experiment-alpha-1-1}
            \end{figure}
            Figure \ref{gradient-descent-2nd-experiment-alpha-1-1} represents the case in which \(\vec{x}^0 = (4,4)^T\) is the starting point: you can easily see that the points computed by the gradient descent method move away from the center of the image (the figure shows just the first 12 results of the algorithm). The problem in this approach is that we're trying to make steps which are too big.
            \item In the third experiment I'm going to use two functions:
            \[f_1(\vec{x}) = x_{1}^{2} + x_{2}^{2}\]
            \[f_2(\vec{x}) = 100(x_{1}^{2} + x_{2}^{2})\]
            You can easily see that, for a given point \(\vec{x} = (x_1, x_2)^T\), both gradients point in the same direction. First, I can try to apply the gradient descent method to both the functions using the same alpha (for example, \(\alpha^k = 0.01\)).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-first-function-alpha-0-0-1.png}
                \caption{Gradient descent method applied to the function \(f_1(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha^k = 0.01\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-first-function-alpha-0-0-1}
            \end{figure}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-second-function-alpha-0-0-1.png}
                \caption{Gradient descent method applied to the function \(f_2(\vec{x}) = 100(x_{1}^{2} + x_{2}^{2})\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha^k = 0.01\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-second-function-alpha-0-0-1}
            \end{figure}
            The results that i get from this experiment are represented in figures \ref{gradient-descent-3rd-experiment-first-function-alpha-0-0-1} and \ref{gradient-descent-3rd-experiment-second-function-alpha-0-0-1}. In the first case (where I'm trying to minimize the \(f_1(\vec{x})\) function), the method slowly converges to the minimum (but it needs more iterations!). In the second case (where I'm trying to minimize the \(f_2(\vec{x})\) function), the method doesn't converge, and I get the sequence \(\vec{x}^0, -\vec{x}^0, \vec{x}^0, -\vec{x}^0, ...\). Looking at these results, I realize that I need different values of \(\alpha^k\) to minimize different functions.\par
            When you have two functions such that their gradients point in the same direction, you can use the normalized gradient as the descent direction:
            \[\vec{x}^{k+1} = \vec{x}^k - \alpha^k \frac{\nabla f(\vec{x}^k)}{\norm{\nabla f(\vec{x}^k)}}\]
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-first-function-normalized-alpha-0-1.png}
                \caption{Gradient descent method applied to the function \(f_1(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha^k = 0.1\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-first-function-normalized-alpha-0-1}
            \end{figure}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-second-function-normalized-alpha-0-1.png}
                \caption{Gradient descent method applied to the function \(f_2(\vec{x}) = 100(x_{1}^{2} + x_{2}^{2})\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha^k = 0.1\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-second-function-normalized-alpha-0-1}
            \end{figure}
            Using such a method, it's possible to use the same \(\alpha^k\) value to minimize both \(f_1(\vec{x})\) and \(f_2(\vec{x})\). You can see an example in figures \ref{gradient-descent-3rd-experiment-first-function-normalized-alpha-0-1} and \ref{gradient-descent-3rd-experiment-second-function-normalized-alpha-0-1}: the normalized gradient descent method is applied to both the functions, using \(\alpha^k = 0.1\).
        \end{enumerate}
    \subsection{The exercise of first laboratory}
        Let's \(\vec{x} \in \R^2\), \(\vec{x} = (x_1, x_2)^T\). I now focus on the following two dimensional function:
        \[f(\vec{x}) = x_1^2(4 - 2.1x_1^2 + \frac{1}{3}x_1^4) + x_1x_2 + x_2^2(-4 + 4x_2^2)\]
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-1st-lab-function-contours.png}
            \caption{Contour plot of the function \(f(\vec{x}) = x_1^2(4 - 2.1x_1^2 + \frac{1}{3}x_1^4) + x_1x_2 + x_2^2(-4 + 4x_2^2)\), with \(x_1 \in [-2, +2]\) and \(x_2 \in [-1, +1]\)}
            \label{gradient-descent-1st-lab-function-contours}
        \end{figure}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-1st-lab-function-surface.png}
            \caption{Graph of the function \(f(\vec{x}) = x_1^2(4 - 2.1x_1^2 + \frac{1}{3}x_1^4) + x_1x_2 + x_2^2(-4 + 4x_2^2)\), with \(x_1 \in [-2, +2]\) and \(x_2 \in [-1, +1]\)}
            \label{gradient-descent-1st-lab-function-surface}
        \end{figure}
        I can plot the function to have an idea of its main features (see figures \ref{gradient-descent-1st-lab-function-contours} and \ref{gradient-descent-1st-lab-function-surface}). You can easily see that the function has several minima. Which minimum will be found using a gradient descent method? It depends on the starting point! I can try to perform a simple experiment to check whether my answer is correct or not: I can apply the gradient descent method to the function, using a constant value of \(\alpha^k\) (\(\alpha^k = 0.05\)) and trying to change the starting point.
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-1st-lab-function-1st-experiment.png}
            \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_1^2(4 - 2.1x_1^2 + \frac{1}{3}x_1^4) + x_1x_2 + x_2^2(-4 + 4x_2^2)\) (with \(x_1 \in [-2, +2]\) and \(x_2 \in [-1, +1]\)), starting from three different points (\(\vec{x}^0 = (-0.6,4)^T\), \(\vec{x}^0 = (0,0.3)^T\) and \(\vec{x}^0 = (-1.1,0)^T\)), using \(\alpha^k = 0.05\) and performing \(100\) iterations}
            \label{gradient-descent-1st-lab-function-1st-experiment}
        \end{figure}
        As you can see in figure \ref{gradient-descent-1st-lab-function-1st-experiment}, depending on the starting points I will have different results.\par
        Let's now perform an improvement to the previous algorithm: I want to try to adapt the value of \(\alpha^k\) at each iteration. A simple backtracking algorithm could try to find a proper value of \(\alpha^k\) in the following way:
        \begin{lstlisting}[language=Python]
            def findAlpha(xk):
                alpha = 1.0
                while function(xk-alpha*function_gradient(xk)) >= function(xk):
                    alpha = alpha / 2
                return alpha
            def gradientDescentWithDynamicAlpha(x0, max_iterations, threshold):
                xk = x0
                alpha = findAlpha(xk)
                xk1 = xk - alpha*function_gradient(xk)
                i = 1
                while (i < max_iterations) &
                      (abs(function(xk1) - function(xk)) > threshold):
                    xk = xk1
                    alpha = findAlpha(xk)
                    xk1 = xk - alpha*function_gradient(xk)
                    i = i + 1
                return np.array(points)
        \end{lstlisting}
        The proposed method stops when \(|f(\vec{x}^{k+1}) - f(\vec{x}^k)| < threshold\), so it could be that it performs less than 100 iterations. Indeed, if I try to apply the algorithm to two different starting point \(\vec{x}_A^0 = (1,0)^T\) and \(\vec{x}_B^0 = (0.6, -0.3)^T\), you can easily see that the method converges faster than the previous one!
        \begin{table}
            \centering
            \begin{tabu}{| c | c | c |}
                \hline
                Starting point &                    Threshold &     Iterations \\ \hline \hline
                \(\vec{x}_A^0 = (1,0)^T\) &         0.001 &         9 \\ \hline
                \(\vec{x}_B^0 = (0.6, -0.3)^T\) &   0.001 &         5 \\ \hline
            \end{tabu}
            \caption{Backtracking gradient descent method applied to the function \(f(\vec{x}) = x_1^2(4 - 2.1x_1^2 + \frac{1}{3}x_1^4) + x_1x_2 + x_2^2(-4 + 4x_2^2)\) starting from two different points and using a fixed threshold}
            \label{gradient-descent-1st-lab-function-2nd-experiment-results}
        \end{table}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-1st-lab-function-2nd-experiment.png}
            \caption{Backtracking gradient descent method applied to the function \(f(\vec{x}) = x_1^2(4 - 2.1x_1^2 + \frac{1}{3}x_1^4) + x_1x_2 + x_2^2(-4 + 4x_2^2)\) (with \(x_1 \in [-2, +2]\) and \(x_2 \in [-1, +1]\)), starting from two different points (\(\vec{x}_A^0 = (1,0)^T\) and \(\vec{x}_B^0 = (0.6, -0.3)^T\). In order to make the figure clearer, only the first points of each application of the algorithm are plotted.}
            \label{gradient-descent-1st-lab-function-2nd-experiment}
        \end{figure}
        See the results in table \ref{gradient-descent-1st-lab-function-2nd-experiment-results} and a graphical representation in figure \ref{gradient-descent-1st-lab-function-2nd-experiment}. It's important to say that this method is not necessarily faster than the normal gradient descent method: it's true that it performs less iterations, but at each iteration it has to compute a proper value of \(\alpha^k\)!