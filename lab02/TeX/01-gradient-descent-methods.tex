\section{Gradient descent methods}
    \subsection{A simple quadratic function}
        Let's \(\vec{x} \in \R^2\), \(\vec{x} = (x_1, x_2)^T\). I start focusing on a two dimensional function:
        \[f(\vec{x}) = x_1^2 + x_2^2\]
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-simple-function-contours.png}
            \caption{Contour plot of the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)}
            \label{gradient-descent-simple-function-contours}
        \end{figure}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-simple-function-surface.png}
            \caption{Graph of the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)}
            \label{gradient-descent-simple-function-surface}
        \end{figure}
        I can plot the function to have an idea of its main features (see figures \ref{gradient-descent-simple-function-contours} and \ref{gradient-descent-simple-function-surface}). You can easily see that there is a local minimum: how can I find it? The easiest way is by performing a gradient descent. How does this method work? You just have to start with a guess \(\vec{x}^0\) for a local minimum of \(f(\vec{x})\) and than you have to take steps proportional to the negative of the gradient of the function at the current point. In other words, you have to consider the sequence \(\vec{x}^0, \vec{x}^1, ..., \vec{x}^k, \vec{x}^{k+1}, ...\) such that
        \[\vec{x}^{k+1} = \vec{x}^k - \alpha^k \nabla f(\vec{x})\]
        \(\alpha^k\) plays an important role since I would like to find the minimum as fast as I can. At the moment, let's keep it constant.\par
        Let's perform now some experiments.
        \begin{enumerate}
            \item In the first experiment I'm going to find a local minimum of the function \(f(\vec{x})\) using the gradient descent method with a constant value \(\alpha^k = 0.1\). I'm also going to try with different starting points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-1st-experiment.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 0.1\) and performing \(100\) iterations}
                \label{gradient-descent-1st-experiment}
            \end{figure}
            As you can see in figure \ref{gradient-descent-1st-experiment}, the gradient descent algorithm converges to the minimum, no matter what the starting point is. It's also possible to see that we don't need \(100\) iterations to get really close to the minimum: in the next experiments we could try to find a way to stop the algorithm when it's close to the real minimum.
            \item In the second experiment I'm going to try different values of \(\alpha\) in order to minimize the function \(f(\vec{x})\) using the gradient descent method. Let's start using \(\alpha = 1\). The starting points are the same that I used in the first experiment: \(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-2nd-experiment-alpha-1.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 1\) and performing \(100\) iterations}
                \label{gradient-descent-2nd-experiment-alpha-1}
            \end{figure}
            As you can see in figure \ref{gradient-descent-2nd-experiment-alpha-1}, the method doesn't converge, no matter what is the starting point. Indeed, if \(\vec{x}^0\) is the starting point, the sequence that you get applying the gradient descent method (with \(\alpha = 1\)) is \(\vec{x}^0, -\vec{x}^0, \vec{x}^0, -\vec{x}^0, ...\). Moreover, if we use \(\alpha = 1.1\) to minimize the function \(f(\vec{x})\), the gradient descent method diverges, moving away from the minimum point.
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-2nd-experiment-alpha-1-1.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from the point \(\vec{x}^0 = (4,4)^T\), using \(\alpha = 1.1\) and performing \(12\) iterations}
                \label{gradient-descent-2nd-experiment-alpha-1-1}
            \end{figure}
            Figure \ref{gradient-descent-2nd-experiment-alpha-1-1} represents the case in which \(\vec{x}^0 = (4,4)^T\) is the starting point: you can easily see that the points computed by the gradient descent method move away from the center of the image (the figure shows just the first 12 results of the algorithm). The problem in this approach is that we're trying to make steps which are too big.
            \item In the third experiment I'm going to use two functions:
            \[f_1(\vec{x}) = x_{1}^{2} + x_{2}^{2}\]
            \[f_2(\vec{x}) = 100(x_{1}^{2} + x_{2}^{2})\]
            You can easily see that, for a given point \(\vec{x} = (x_1, x_2)^T\), both gradients point in the same direction. First, I can try to apply the gradient descent method to both the functions using the same alpha (for example, \(\alpha = 0.01\)).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-first-function-alpha-0-0-1.png}
                \caption{Gradient descent method applied to the function \(f_1(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 0.01\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-first-function-alpha-0-0-1}
            \end{figure}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-second-function-alpha-0-0-1.png}
                \caption{Gradient descent method applied to the function \(f_2(\vec{x}) = 100(x_{1}^{2} + x_{2}^{2})\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 0.01\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-second-function-alpha-0-0-1}
            \end{figure}
            The results that i get from this experiment are represented in figures \ref{gradient-descent-3rd-experiment-first-function-alpha-0-0-1} and \ref{gradient-descent-3rd-experiment-second-function-alpha-0-0-1}. In the first case (where I'm trying to minimize the \(f_1(\vec{x})\) function), the method slowly converges to the minimum (but it needs more iterations!). In the second case (where I'm trying to minimize the \(f_2(\vec{x})\) function), the method doesn't converge, and I get the sequence \(\vec{x}^0, -\vec{x}^0, \vec{x}^0, -\vec{x}^0, ...\). Looking at these results, I realize that I need different values of \(\alpha\) to minimize different functions.\par
            When you have two functions such that their gradients point in the same direction, you can use the normalized gradient as the descent direction:
            \[\vec{x}^{k+1} = \vec{x}^k - \alpha^k \frac{\nabla f(\vec{x}^k)}{\norm{\nabla f(\vec{x}^k)}}\]
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-first-function-normalized-alpha-0-1.png}
                \caption{Gradient descent method applied to the function \(f_1(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 0.1\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-first-function-normalized-alpha-0-1}
            \end{figure}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-3rd-experiment-second-function-normalized-alpha-0-1.png}
                \caption{Gradient descent method applied to the function \(f_2(\vec{x}) = 100(x_{1}^{2} + x_{2}^{2})\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 0.1\) and performing \(100\) iterations}
                \label{gradient-descent-3rd-experiment-second-function-normalized-alpha-0-1}
            \end{figure}
            Using such a method, it's possible to use the same \(\alpha\) value to minimize both \(f_1(\vec{x})\) and \(f_2(\vec{x})\). You can see an example in figures \ref{gradient-descent-3rd-experiment-first-function-normalized-alpha-0-1} and \ref{gradient-descent-3rd-experiment-second-function-normalized-alpha-0-1}: the normalized gradient descent method is applied to both the functions, using \(\alpha = 0.1\).
        \end{enumerate}