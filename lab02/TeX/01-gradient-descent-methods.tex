\section{Gradient descent methods}
    \subsection{A simple quadratic function}
        Let's \(\vec{x} \in \R^2\), \(\vec{x} = (x_1, x_2)^T\). I start focusing on a two dimensional function:
        \[f(\vec{x}) = x_1^2 + x_2^2\]
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-simple-function-contours.png}
            \caption{Contour plot of the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)}
            \label{gradient-descent-simple-function-contours}
        \end{figure}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-simple-function-surface.png}
            \caption{Graph of the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)}
            \label{gradient-descent-simple-function-surface}
        \end{figure}
        I can plot the function to have an idea of its main features (see figures \ref{gradient-descent-simple-function-contours} and \ref{gradient-descent-simple-function-surface}). You can easily see that there is a local minimum: how can I find it? The easiest way is by performing a gradient descent. How does this method work? You just have to start with a guess \(\vec{x}^0\) for a local minimum of \(f(\vec{x})\) and than you have to take steps proportional to the negative of the gradient of the function at the current point. In other words, you have to consider the sequence \(x^0, x^1, ..., x^k, x^{k+1}, ...\) such that
        \[x^{k+1} = x^k - \alpha^k \nabla f(\vec{x})\]
        \(\alpha^k\) plays an important role since I would like to find the minimum as fast as I can. At the moment, let's keep it constant.\par
        Let's perform now some experiments.
        \begin{enumerate}
            \item In the first experiment I'm going to find a local minimum of the function \(f(\vec{x})\) using the gradient descent method with a constant value \(\alpha^k = 0.1\). I'm also going to try with different starting points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-1st-experiment.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 0.1\) and performing \(100\) iterations}
                \label{gradient-descent-1st-experiment}
            \end{figure}
            As you can see in figure \ref{gradient-descent-1st-experiment}, the gradient descent algorithm converges to the minimum, no matter what the starting point is. It's also possible to see that we don't need \(100\) iterations to get really close to the minimum: in the next experiments we could try to find a way to stop the algorithm when it's close to the real minimum.
            \item In the second experiment I'm going to try different values of \(\alpha\) in order to minimize the function \(f(\vec{x})\) using the gradient descent method. Let's start using \(\alpha = 1\). The starting points are the same that I used in the first experiment: \(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\).
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-2nd-experiment-alpha-1.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from three different points (\(\vec{x}^0 = (4,4)^T\), \(\vec{x}^0 = (-3,2)^T\) and \(\vec{x}^0 = (4,-3)^T\)), using \(\alpha = 1\) and performing \(100\) iterations}
                \label{gradient-descent-2nd-experiment-alpha-1}
            \end{figure}
            As you can see in figure \ref{gradient-descent-2nd-experiment-alpha-1}, the method doesn't converge, no matter what is the starting point. Indeed, if \(\vec{x}^0\) is the starting point, the sequence that you get applying the gradient descent method (with \(\alpha = 1\)) is \(\vec{x}^0, -\vec{x}^0, \vec{x}^0, -\vec{x}^0, ...\). Moreover, if we use \(\alpha = 1.1\) to minimize the function \(f(\vec{x})\), the gradient descent method diverges, moving away from the minimum point.
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/01-gradient-descent-2nd-experiment-alpha-1-1.png}
                \caption{Gradient descent method applied to the function \(f(\vec{x}) = x_{1}^{2} + x_{2}^{2}\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-5, +5]\)), starting from the point \(\vec{x}^0 = (4,4)^T\), using \(\alpha = 1.1\) and performing \(12\) iterations}
                \label{gradient-descent-2nd-experiment-alpha-1-1}
            \end{figure}
            Figure \ref{gradient-descent-2nd-experiment-alpha-1-1} represents the case in which \(\vec{x}^0 = (4,4)^T\) is the starting point: you can easily see that the points computed by the gradient descent method move away from the center of the image (the figure shows just the first 12 results of the algorithm). The problem in this approach is that we're trying to make steps which are too big.
        \end{enumerate}