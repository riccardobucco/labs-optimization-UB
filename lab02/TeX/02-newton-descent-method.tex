\section{Newton descent method}
    We have seen two different search directions in the descent methods: the negative of the gradient and the negative of the normalized gradient. What other search directions may we take? One well known search direction is the Newton one.\par
    In a general case, an unconstrained minimization algorithm uses the next algorithm:
    \[\vec{x}^{k+1} = \vec{x}^k + \alpha^k\vec{d}^k\]
    where the vector \(\vec{d}^k\) is a descent direction (also called search direction). For the Newton method the descent direction \(\vec{d}^k\) is the solution of
    \[\nabla^2f(\vec{x}^k)\vec{d}^k = -\nabla f(\vec{x}^k)\]
    The above equation can be solved as follows:
    \[\vec{d}^k = (\nabla^2f(\vec{x}^k))^{-1}(-\nabla f(\vec{x}^k))\]
    As you can easily see, the Newton descent method can be applied only if the Hessian matrix of \(f(\vec{x}^k)\) is invertible.
    \subsection{A simple quadratic function}
        Let's \(\vec{x} \in \R^2\), \(\vec{x} = (x_1, x_2)^T\). I start by focusing on a simple two dimensional quadratic function:
        \[f(\vec{x}) = 100x_1^2 + x_2^2\]
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/02-simple-quadratic-function-countours.png}
            \caption{Contour plot of the function \(f(\vec{x}) = 100x_1^2 + x_2^2\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-50, +50]\)}
            \label{simple-quadratic-function-countours}
        \end{figure}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/02-simple-quadratic-function-surface.png}
            \caption{Graph of the function \(f(\vec{x}) = 100x_1^2 + x_2^2\), with \(x_1 \in [-5, +5]\) and \(x_2 \in [-50, +50]\)}
            \label{simple-quadratic-function-surface}
        \end{figure}
        I can plot the function to have an idea of its main features (see figures \ref{simple-quadratic-function-countours} and \ref{simple-quadratic-function-surface}). As you can easily see, this function is convex, and thus it has a unique stationary point which corresponds to the minimum.\par
        Given a certain starting point \(\vec{x}^0\), I can now try to use the gradient descent algorithm (using the backtracking algorithm that I previously developed in order to find a proper value of \(\alpha^k\) at each iteration) to count the number of steps that are needed in order to find the minimum so that \(|f(\vec{x}^{k+1}) - f(\vec{x}^k)| < 10^{-3}\). After that, I can do the same using the Newton algorithm: in this way it's possible to compare the two different descent methods. Notice that the Newton method just chooses the direction, while the step \(\alpha^k\) is computed at each iteration using the backtracking approach.
        \begin{table}
            \centering
            \begin{tabu}{| c | c | c | c |}
                \hline
                \multirow{2}{*}{Starting point} &       \multirow{2}{*}{Threshold} &    \multicolumn{2}{|c|}{Iterations}    \\ \cline{3-4}
                &                                       &                               Backtracking &                      Newton \\ \hline \hline 
                \(\vec{x}_A^0 = (4,40)^T\) &            0.001 &                         209 &                               2 \\ \hline
                \(\vec{x}_B^0 = (-2,-20)^T\) &          0.001 &                         185 &                               2 \\ \hline
            \end{tabu}
            \caption{Comparison between the gradient descent method and the Newton descent method: both the algorithms use the backtracking approach to find at each iteration a proper value of \(\alpha^k\), they are both applied to the function \(f(\vec{x}) = 100x_1^2 + x_2^2\) starting from two different points and using a fixed threshold}
            \label{comparison-gradient-newton-results}
        \end{table}
        As you can see in table \ref{comparison-gradient-newton-results}, the Newton method is definitely faster, compared with the gradient descent method.
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/02-simple-quadratic-function-backtracking.png}
            \caption{Gradient descent method (using a backtracking approach to find a proper value of \(\alpha^k\) at each iteration) applied to the function \(f(\vec{x}) = 100x_1^2 + x_2^2\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-50, +50]\)), starting from two points \(\vec{x}_A^0 = (4,40)^T\) and \(\vec{x}_B^0 = (-2,-20)^T\), and using a fixed threshold}
            \label{simple-quadratic-function-backtracking}
        \end{figure}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{../Images/02-simple-quadratic-function-newton.png}
            \caption{Newton descent method (using a backtracking approach to find a proper value of \(\alpha^k\) at each iteration) applied to the function \(f(\vec{x}) = 100x_1^2 + x_2^2\) (with \(x_1 \in [-5, +5]\) and \(x_2 \in [-50, +50]\)), starting from two points \(\vec{x}_A^0 = (4,40)^T\) and \(\vec{x}_B^0 = (-2,-20)^T\), and using a fixed threshold}
            \label{simple-quadratic-function-newton}
        \end{figure}
        If you try to plot the path that each method follows in order to arrive to the minimum (see figures \ref{simple-quadratic-function-backtracking} and \ref{simple-quadratic-function-newton}), you can easily see that the direction chosen by the Newton method is the best one, while the gradient descent method takes many non optimal directions. The Newton method is really fast in this case because the function \(f(\vec{x})\) we are studying is quadratic, and the Newton algorithm approximates it with a quadratic function (that is, the approximation is perfect).
        \subsection{The exercise of first laboratory}
            Let's \(\vec{x} \in \R^2\), \(\vec{x} = (x_1, x_2)^T\). I now focus on the following two dimensional function:
            \[f(\vec{x}) = x_1^2(4 - 2.1x_1^2 + \frac{1}{3}x_1^4) + x_1x_2 + x_2^2(-4 + 4x_2^2)\]
            Check figures \ref{gradient-descent-1st-lab-function-contours} and \ref{gradient-descent-1st-lab-function-surface} to have an idea of the main features of this function: it is easy to see that the function is not convex and has several minima. I can now try to compare the Newton descent method and the gradient descent method, applying both the algorithms to the function introduced in the first laboratory. There is one problem: it is not always possible to use the Newton method, because the Hessian matrix is not always positive definite. Thus, another approach has to be used if the Hessian is not positive definite. In the next experiment, I'll try to apply the gradient descent method every time that is not possible to apply the Newton method. How can I check if a given matrix is positive definite or not? You just have to compute its eigenvalues and check whether they are all positive or not.\par
            \begin{lstlisting}[language=Python]
                def findAlpha(xk):
                    alpha = 1.0
                    while function(xk-alpha*gradient(xk)) >= function(xk):
                        alpha = alpha / 2
                    return alpha
                def findAlphaNewton(xk):
                    alpha = 1.0
                    while function(xk + alpha*inv(hessian(xk))*(-gradient(xk))) >
                          function(xk):
                        alpha = alpha / 2
                    return alpha
                def newtonGradientDescent(x0, max_iterations, threshold):
                    xk = x0
                    if isPositiveDefinite(hessian(xk)):
                        alpha = findAlphaNewton(xk)
                        xk1 = xk + alpha*inv(hessian(xk))*(-gradient(xk))
                    else:
                        alpha = findAlpha(xk)
                        xk1 = xk - alpha*gradient(xk)
                    i = 1
                    while (i < max_iterations) &
                          (abs(function(xk1) - function(xk)) > threshold):
                        xk = xk1
                        if isPositiveDefinite(hessian(xk)):
                            alpha = findAlphaNewton(xk)
                            xk1 = xk + alpha*inv(hessian(xk))*(-gradient(xk))
                        else:
                            alpha = findAlpha(xk)
                            xk1 = xk - alpha*gradient(xk)
                        i = i + 1
            \end{lstlisting}
            If you try to apply this algorithm to find the minimum, you can easily see that there are high chances that the Newton method is applied when you get close to the minimum. Moreover, this method is clearly faster than the Gradient descent method: indeed, you always apply the Newton method (which is usually faster than the gradient one), except in those cases in which it is not possible to apply it.\par
            You should note that even if this method usually needs less iterations, it has to compute whether a matrix is positive definite and it has also to invert it! Therefor, for some problems the gradient descent method could be the best option!
        \subsection{The Rosenbrock function}
            Let's \(\vec{x} \in \R^2\), \(\vec{x} = (x_1, x_2)^T\), \(a,b \in \R\). I now focus on the following two dimensional function (Rosenbrock function):
            \[f(\vec{x}) = (a - x_1)^2 + b(x_2 - x_1^2)^2\]
            The function has a global minimum at \((x_1, x_2) = (a, a^2)\). This minimum is inside a long, narrow, parabolic shaped valley. Let's try to study the case in which \(a=1\) and \(b=100\).\par
            First of all, in order to have an idea of the main features of the function you can look at figures \ref{rosenbrock-function-surface} and \ref{rosenbrock-function-contours}. You can also check table \ref{backtracking-algorithm-experiments-results}, which reports the results that I got applying the gradient descent method to the Rosenbrock function.
            \begin{figure}
                \centering
                \includegraphics[width=0.7\textwidth]{../Images/02-rosenbrock-function-newton.png}
                \caption{Newton/Gradient descent method applied to the function \(f(\vec{x}) = (a - x_1)^2 + b(x_2 - x_1^2)^2\) (with \(x_1 \in [-2, +2]\), \(x_2 \in [-1, +3]\), \(a=1\) and \(b=100\)), starting from the point \(\vec{x}_A^0 = (-1.5,-0.5)^T\) and using a threshold of \(0.001\) to stop the algorithm. The red points indicate that the gradient method is used, the green ones indicate that the Newton method is used. The yellow point represents the real minimum of the function.}
                \label{rosenbrock-function-newton}
            \end{figure}
            If you try to apply the developed method (using Newton descent whenever it's possible, that is whenever the Hessian is positive definite) you can easily see that the method is really fast, compared with the normal gradient descent algorithm! Look at the figure \ref{rosenbrock-function-newton} to see the behavior of the proposed method. From the figure you can see that the gradient descent method is used to arrive into the valley of the function; after that, the Newton method is used to cross the valley and arrive to the desired minimum.